{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/ececis_research/peace/sw/anaconda-envs/ddrl/20231005/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from scripts.util.detection.nsfw_and_watermark_dectection import \\\n",
    "    DeepFloydDataFiltering\n",
    "from sgm.inference.helpers import embed_watermark\n",
    "from sgm.util import default, instantiate_from_config\n",
    "\n",
    "\n",
    "def sample(\n",
    "    input_path: str = \"assets/test_image.png\",  # Can either be image file or folder with image files\n",
    "    num_frames: Optional[int] = None,\n",
    "    num_steps: Optional[int] = None,\n",
    "    version: str = \"svd\",\n",
    "    fps_id: int = 6,\n",
    "    motion_bucket_id: int = 127,\n",
    "    cond_aug: float = 0.02,\n",
    "    seed: int = 23,\n",
    "    decoding_t: int = 14,  # Number of frames decoded at a time! This eats most VRAM. Reduce if necessary.\n",
    "    device: str = \"cuda\",\n",
    "    output_folder: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simple script to generate a single sample conditioned on an image `input_path` or multiple images, one for each\n",
    "    image file in folder `input_path`. If you run out of VRAM, try decreasing `decoding_t`.\n",
    "    \"\"\"\n",
    "\n",
    "    if version == \"svd\":\n",
    "        num_frames = default(num_frames, 14)\n",
    "        num_steps = default(num_steps, 25)\n",
    "        output_folder = default(output_folder, \"outputs/simple_video_sample/svd/\")\n",
    "        model_config = \"scripts/sampling/configs/svd.yaml\"\n",
    "    elif version == \"svd_xt\":\n",
    "        num_frames = default(num_frames, 25)\n",
    "        num_steps = default(num_steps, 30)\n",
    "        output_folder = default(output_folder, \"outputs/simple_video_sample/svd_xt/\")\n",
    "        model_config = \"scripts/sampling/configs/svd_xt.yaml\"\n",
    "    elif version == \"svd_image_decoder\":\n",
    "        num_frames = default(num_frames, 14)\n",
    "        num_steps = default(num_steps, 25)\n",
    "        output_folder = default(\n",
    "            output_folder, \"outputs/simple_video_sample/svd_image_decoder/\"\n",
    "        )\n",
    "        model_config = \"scripts/sampling/configs/svd_image_decoder.yaml\"\n",
    "    elif version == \"svd_xt_image_decoder\":\n",
    "        num_frames = default(num_frames, 25)\n",
    "        num_steps = default(num_steps, 30)\n",
    "        output_folder = default(\n",
    "            output_folder, \"outputs/simple_video_sample/svd_xt_image_decoder/\"\n",
    "        )\n",
    "        model_config = \"scripts/sampling/configs/svd_xt_image_decoder.yaml\"\n",
    "    else:\n",
    "        raise ValueError(f\"Version {version} does not exist.\")\n",
    "\n",
    "    model, filter = load_model(\n",
    "        model_config,\n",
    "        device,\n",
    "        num_frames,\n",
    "        num_steps,\n",
    "    )\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    path = Path(input_path)\n",
    "    all_img_paths = []\n",
    "    if path.is_file():\n",
    "        if any([input_path.endswith(x) for x in [\"jpg\", \"jpeg\", \"png\"]]):\n",
    "            all_img_paths = [input_path]\n",
    "        else:\n",
    "            raise ValueError(\"Path is not valid image file.\")\n",
    "    elif path.is_dir():\n",
    "        all_img_paths = sorted(\n",
    "            [\n",
    "                f\n",
    "                for f in path.iterdir()\n",
    "                if f.is_file() and f.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n",
    "            ]\n",
    "        )\n",
    "        if len(all_img_paths) == 0:\n",
    "            raise ValueError(\"Folder does not contain any images.\")\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    for input_img_path in all_img_paths:\n",
    "        with Image.open(input_img_path) as image:\n",
    "            if image.mode == \"RGBA\":\n",
    "                image = image.convert(\"RGB\")\n",
    "            w, h = image.size\n",
    "\n",
    "            if h % 64 != 0 or w % 64 != 0:\n",
    "                width, height = map(lambda x: x - x % 64, (w, h))\n",
    "                image = image.resize((width, height))\n",
    "                print(\n",
    "                    f\"WARNING: Your image is of size {h}x{w} which is not divisible by 64. We are resizing to {height}x{width}!\"\n",
    "                )\n",
    "\n",
    "            image = ToTensor()(image)\n",
    "            image = image * 2.0 - 1.0\n",
    "\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        H, W = image.shape[2:]\n",
    "        assert image.shape[1] == 3\n",
    "        F = 8\n",
    "        C = 4\n",
    "        shape = (num_frames, C, H // F, W // F)\n",
    "        if (H, W) != (576, 1024):\n",
    "            print(\n",
    "                \"WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\"\n",
    "            )\n",
    "        if motion_bucket_id > 255:\n",
    "            print(\n",
    "                \"WARNING: High motion bucket! This may lead to suboptimal performance.\"\n",
    "            )\n",
    "\n",
    "        if fps_id < 5:\n",
    "            print(\"WARNING: Small fps value! This may lead to suboptimal performance.\")\n",
    "\n",
    "        if fps_id > 30:\n",
    "            print(\"WARNING: Large fps value! This may lead to suboptimal performance.\")\n",
    "\n",
    "        value_dict = {}\n",
    "        value_dict[\"motion_bucket_id\"] = motion_bucket_id\n",
    "        value_dict[\"fps_id\"] = fps_id\n",
    "        value_dict[\"cond_aug\"] = cond_aug\n",
    "        value_dict[\"cond_frames_without_noise\"] = image\n",
    "        value_dict[\"cond_frames\"] = image + cond_aug * torch.randn_like(image)\n",
    "        value_dict[\"cond_aug\"] = cond_aug\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device):\n",
    "                batch, batch_uc = get_batch(\n",
    "                    get_unique_embedder_keys_from_conditioner(model.conditioner),\n",
    "                    value_dict,\n",
    "                    [1, num_frames],\n",
    "                    T=num_frames,\n",
    "                    device=device,\n",
    "                )\n",
    "                c, uc = model.conditioner.get_unconditional_conditioning(\n",
    "                    batch,\n",
    "                    batch_uc=batch_uc,\n",
    "                    force_uc_zero_embeddings=[\n",
    "                        \"cond_frames\",\n",
    "                        \"cond_frames_without_noise\",\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                for k in [\"crossattn\", \"concat\"]:\n",
    "                    uc[k] = repeat(uc[k], \"b ... -> b t ...\", t=num_frames)\n",
    "                    uc[k] = rearrange(uc[k], \"b t ... -> (b t) ...\", t=num_frames)\n",
    "                    c[k] = repeat(c[k], \"b ... -> b t ...\", t=num_frames)\n",
    "                    c[k] = rearrange(c[k], \"b t ... -> (b t) ...\", t=num_frames)\n",
    "\n",
    "                randn = torch.randn(shape, device=device)\n",
    "\n",
    "                additional_model_inputs = {}\n",
    "                additional_model_inputs[\"image_only_indicator\"] = torch.zeros(\n",
    "                    2, num_frames\n",
    "                ).to(device)\n",
    "                additional_model_inputs[\"num_video_frames\"] = batch[\"num_video_frames\"]\n",
    "\n",
    "                def denoiser(input, sigma, c):\n",
    "                    return model.denoiser(\n",
    "                        model.model, input, sigma, c, **additional_model_inputs\n",
    "                    )\n",
    "\n",
    "                samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n",
    "                model.en_and_decode_n_samples_a_time = decoding_t\n",
    "                samples_x = model.decode_first_stage(samples_z)\n",
    "                samples = torch.clamp((samples_x + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "                base_count = len(glob(os.path.join(output_folder, \"*.mp4\")))\n",
    "                video_path = os.path.join(output_folder, f\"{base_count:06d}.mp4\")\n",
    "                writer = cv2.VideoWriter(\n",
    "                    video_path,\n",
    "                    cv2.VideoWriter_fourcc(*\"MP4V\"),\n",
    "                    fps_id + 1,\n",
    "                    (samples.shape[-1], samples.shape[-2]),\n",
    "                )\n",
    "\n",
    "                samples = embed_watermark(samples)\n",
    "#                 samples = filter(samples)\n",
    "                vid = (\n",
    "                    (rearrange(samples, \"t c h w -> t h w c\") * 255)\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                    .astype(np.uint8)\n",
    "                )\n",
    "                for frame in vid:\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                    writer.write(frame)\n",
    "                writer.release()\n",
    "\n",
    "\n",
    "def get_unique_embedder_keys_from_conditioner(conditioner):\n",
    "    return list(set([x.input_key for x in conditioner.embedders]))\n",
    "\n",
    "\n",
    "def get_batch(keys, value_dict, N, T, device):\n",
    "    batch = {}\n",
    "    batch_uc = {}\n",
    "\n",
    "    for key in keys:\n",
    "        if key == \"fps_id\":\n",
    "            batch[key] = (\n",
    "                torch.tensor([value_dict[\"fps_id\"]])\n",
    "                .to(device)\n",
    "                .repeat(int(math.prod(N)))\n",
    "            )\n",
    "        elif key == \"motion_bucket_id\":\n",
    "            batch[key] = (\n",
    "                torch.tensor([value_dict[\"motion_bucket_id\"]])\n",
    "                .to(device)\n",
    "                .repeat(int(math.prod(N)))\n",
    "            )\n",
    "        elif key == \"cond_aug\":\n",
    "            batch[key] = repeat(\n",
    "                torch.tensor([value_dict[\"cond_aug\"]]).to(device),\n",
    "                \"1 -> b\",\n",
    "                b=math.prod(N),\n",
    "            )\n",
    "        elif key == \"cond_frames\":\n",
    "            batch[key] = repeat(value_dict[\"cond_frames\"], \"1 ... -> b ...\", b=N[0])\n",
    "        elif key == \"cond_frames_without_noise\":\n",
    "            batch[key] = repeat(\n",
    "                value_dict[\"cond_frames_without_noise\"], \"1 ... -> b ...\", b=N[0]\n",
    "            )\n",
    "        else:\n",
    "            batch[key] = value_dict[key]\n",
    "\n",
    "    if T is not None:\n",
    "        batch[\"num_video_frames\"] = T\n",
    "\n",
    "    for key in batch.keys():\n",
    "        if key not in batch_uc and isinstance(batch[key], torch.Tensor):\n",
    "            batch_uc[key] = torch.clone(batch[key])\n",
    "    return batch, batch_uc\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    config: str,\n",
    "    device: str,\n",
    "    num_frames: int,\n",
    "    num_steps: int,\n",
    "):\n",
    "    config = OmegaConf.load(config)\n",
    "    if device == \"cuda\":\n",
    "        config.model.params.conditioner_config.params.emb_models[\n",
    "            0\n",
    "        ].params.open_clip_embedding_config.params.init_device = device\n",
    "\n",
    "    config.model.params.sampler_config.params.num_steps = num_steps\n",
    "    config.model.params.sampler_config.params.guider_config.params.num_frames = (\n",
    "        num_frames\n",
    "    )\n",
    "    if device == \"cuda\":\n",
    "        with torch.device(device):\n",
    "            model = instantiate_from_config(config.model).to(device).eval()\n",
    "    else:\n",
    "        model = instantiate_from_config(config.model).to(device).eval()\n",
    "\n",
    "#     filter = DeepFloydDataFiltering(verbose=False, device=device)\n",
    "    filter = lambda x : x\n",
    "    return model, filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "Initialized embedder #0: FrozenOpenCLIPImagePredictionEmbedder with 683800065 params. Trainable: False\n",
      "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #3: VideoPredictionEmbedderWithEncoder with 83653863 params. Trainable: False\n",
      "Initialized embedder #4: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Restored from checkpoints/svd_xt.safetensors with 0 missing and 0 unexpected keys\n",
      "WARNING: Your image is of size 1198x868 which is not divisible by 64. We are resizing to 1152x832!\n",
      "WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/ececis_research/peace/sw/anaconda-envs/ddrl/20231005/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/work/ececis_research/peace/sw/anaconda-envs/ddrl/20231005/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "image_path = \"fire-hydrant-1.jpeg\"\n",
    "sample(image_path, version= \"svd_xt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "Initialized embedder #0: FrozenOpenCLIPImagePredictionEmbedder with 683800065 params. Trainable: False\n",
      "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #3: VideoPredictionEmbedderWithEncoder with 83653863 params. Trainable: False\n",
      "Initialized embedder #4: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Restored from checkpoints/svd_xt.safetensors with 0 missing and 0 unexpected keys\n",
      "WARNING: Your image is of size 1198x868 which is not divisible by 64. We are resizing to 1152x832!\n",
      "WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "Initialized embedder #0: FrozenOpenCLIPImagePredictionEmbedder with 683800065 params. Trainable: False\n",
      "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #3: VideoPredictionEmbedderWithEncoder with 83653863 params. Trainable: False\n",
      "Initialized embedder #4: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Restored from checkpoints/svd_xt.safetensors with 0 missing and 0 unexpected keys\n",
      "WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "Initialized embedder #0: FrozenOpenCLIPImagePredictionEmbedder with 683800065 params. Trainable: False\n",
      "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #3: VideoPredictionEmbedderWithEncoder with 83653863 params. Trainable: False\n",
      "Initialized embedder #4: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Restored from checkpoints/svd_xt.safetensors with 0 missing and 0 unexpected keys\n",
      "WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Initial image path\u001b[39;00m\n\u001b[1;32m     34\u001b[0m initial_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfire-hydrant-1.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mrepeated_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_image_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36mrepeated_sampling\u001b[0;34m(image_path, iterations, version)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepeated_sampling\u001b[39m(image_path, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_xt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[0;32m---> 25\u001b[0m         \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m         output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs/simple_video_sample/svd_xt/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m         latest_video \u001b[38;5;241m=\u001b[39m get_latest_video(output_folder)\n",
      "Cell \u001b[0;32mIn[1], line 176\u001b[0m, in \u001b[0;36msample\u001b[0;34m(input_path, num_frames, num_steps, version, fps_id, motion_bucket_id, cond_aug, seed, decoding_t, device, output_folder)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdenoiser\u001b[39m(\u001b[38;5;28minput\u001b[39m, sigma, c):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mdenoiser(\n\u001b[1;32m    173\u001b[0m         model\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28minput\u001b[39m, sigma, c, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madditional_model_inputs\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 176\u001b[0m samples_z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdenoiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m model\u001b[38;5;241m.\u001b[39men_and_decode_n_samples_a_time \u001b[38;5;241m=\u001b[39m decoding_t\n\u001b[1;32m    178\u001b[0m samples_x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode_first_stage(samples_z)\n",
      "File \u001b[0;32m~/dino-diffusion/stable-video-diffusion/sgm/modules/diffusionmodules/sampling.py:120\u001b[0m, in \u001b[0;36mEDMSampler.__call__\u001b[0;34m(self, denoiser, x, cond, uc, num_steps)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sigma_gen(num_sigmas):\n\u001b[1;32m    115\u001b[0m     gamma \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_churn \u001b[38;5;241m/\u001b[39m (num_sigmas \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_tmin \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sigmas[i] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_tmax\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    119\u001b[0m     )\n\u001b[0;32m--> 120\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms_in\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms_in\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdenoiser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43muc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/dino-diffusion/stable-video-diffusion/sgm/modules/diffusionmodules/sampling.py:99\u001b[0m, in \u001b[0;36mEDMSampler.sampler_step\u001b[0;34m(self, sigma, next_sigma, denoiser, x, cond, uc, gamma)\u001b[0m\n\u001b[1;32m     96\u001b[0m     eps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_noise\n\u001b[1;32m     97\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m eps \u001b[38;5;241m*\u001b[39m append_dims(sigma_hat\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m sigma\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, x\u001b[38;5;241m.\u001b[39mndim) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m---> 99\u001b[0m denoised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdenoise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenoiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m d \u001b[38;5;241m=\u001b[39m to_d(x, sigma_hat, denoised)\n\u001b[1;32m    101\u001b[0m dt \u001b[38;5;241m=\u001b[39m append_dims(next_sigma \u001b[38;5;241m-\u001b[39m sigma_hat, x\u001b[38;5;241m.\u001b[39mndim)\n",
      "File \u001b[0;32m~/dino-diffusion/stable-video-diffusion/sgm/modules/diffusionmodules/sampling.py:55\u001b[0m, in \u001b[0;36mBaseDiffusionSampler.denoise\u001b[0;34m(self, x, denoiser, sigma, cond, uc)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdenoise\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, denoiser, sigma, cond, uc):\n\u001b[0;32m---> 55\u001b[0m     denoised \u001b[38;5;241m=\u001b[39m \u001b[43mdenoiser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     denoised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguider(denoised, sigma)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m denoised\n",
      "Cell \u001b[0;32mIn[1], line 172\u001b[0m, in \u001b[0;36msample.<locals>.denoiser\u001b[0;34m(input, sigma, c)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdenoiser\u001b[39m(\u001b[38;5;28minput\u001b[39m, sigma, c):\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdenoiser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_model_inputs\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sw/anaconda-envs/ddrl/20231005/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sw/anaconda-envs/ddrl/20231005/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dino-diffusion/stable-video-diffusion/sgm/modules/diffusionmodules/denoiser.py:37\u001b[0m, in \u001b[0;36mDenoiser.forward\u001b[0;34m(self, network, input, sigma, cond, **additional_model_inputs)\u001b[0m\n\u001b[1;32m     34\u001b[0m c_skip, c_out, c_in, c_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling(sigma)\n\u001b[1;32m     35\u001b[0m c_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpossibly_quantize_c_noise(c_noise\u001b[38;5;241m.\u001b[39mreshape(sigma_shape))\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_model_inputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m c_out\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m c_skip\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m~/sw/anaconda-envs/ddrl/20231005/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sw/anaconda-envs/ddrl/20231005/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dino-diffusion/stable-video-diffusion/sgm/modules/diffusionmodules/wrappers.py:28\u001b[0m, in \u001b[0;36mOpenAIWrapper.forward\u001b[0;34m(self, x, t, c, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, t: torch\u001b[38;5;241m.\u001b[39mTensor, c: \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, c\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor([])\u001b[38;5;241m.\u001b[39mtype_as(x))), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrossattn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvector\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sw/anaconda-envs/ddrl/20231005/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sw/anaconda-envs/ddrl/20231005/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dino-diffusion/stable-video-diffusion/sgm/modules/diffusionmodules/video_model.py:465\u001b[0m, in \u001b[0;36mVideoUNet.forward\u001b[0;34m(self, x, timesteps, context, y, time_context, num_video_frames, image_only_indicator)\u001b[0m\n\u001b[1;32m    463\u001b[0m h \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_blocks:\n\u001b[0;32m--> 465\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_only_indicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_only_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_video_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_video_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m     hs\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[1;32m    474\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle_block(\n\u001b[1;32m    475\u001b[0m     h,\n\u001b[1;32m    476\u001b[0m     emb,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m     num_video_frames\u001b[38;5;241m=\u001b[39mnum_video_frames,\n\u001b[1;32m    481\u001b[0m )\n",
      "File \u001b[0;32m~/sw/anaconda-envs/ddrl/20231005/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sw/anaconda-envs/ddrl/20231005/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dino-diffusion/stable-video-diffusion/sgm/modules/diffusionmodules/openaimodel.py:93\u001b[0m, in \u001b[0;36mTimestepEmbedSequential.forward\u001b[0;34m(self, x, emb, context, image_only_indicator, time_context, num_video_frames)\u001b[0m\n\u001b[1;32m     91\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, emb, num_video_frames, image_only_indicator)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, SpatialVideoTransformer):\n\u001b[0;32m---> 93\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_video_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_only_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, SpatialTransformer):\n\u001b[1;32m    101\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, context)\n",
      "File \u001b[0;32m~/sw/anaconda-envs/ddrl/20231005/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sw/anaconda-envs/ddrl/20231005/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dino-diffusion/stable-video-diffusion/sgm/modules/video_attention.py:269\u001b[0m, in \u001b[0;36mSpatialVideoTransformer.forward\u001b[0;34m(self, x, context, time_context, timesteps, image_only_indicator)\u001b[0m\n\u001b[1;32m    267\u001b[0m num_frames \u001b[38;5;241m=\u001b[39m repeat(num_frames, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt -> b t\u001b[39m\u001b[38;5;124m\"\u001b[39m, b\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m timesteps)\n\u001b[1;32m    268\u001b[0m num_frames \u001b[38;5;241m=\u001b[39m rearrange(num_frames, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb t -> (b t)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 269\u001b[0m t_emb \u001b[38;5;241m=\u001b[39m \u001b[43mtimestep_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_time_embed_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_pos_embed(t_emb)\n\u001b[1;32m    276\u001b[0m emb \u001b[38;5;241m=\u001b[39m emb[:, \u001b[38;5;28;01mNone\u001b[39;00m, :]\n",
      "File \u001b[0;32m~/dino-diffusion/stable-video-diffusion/sgm/modules/diffusionmodules/util.py:218\u001b[0m, in \u001b[0;36mtimestep_embedding\u001b[0;34m(timesteps, dim, max_period, repeat_only)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m repeat_only:\n\u001b[1;32m    217\u001b[0m     half \u001b[38;5;241m=\u001b[39m dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 218\u001b[0m     freqs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_period\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhalf\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     args \u001b[38;5;241m=\u001b[39m timesteps[:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m*\u001b[39m freqs[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    224\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mcos(args), torch\u001b[38;5;241m.\u001b[39msin(args)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_latest_video(output_folder):\n",
    "    video_files = sorted(Path(output_folder).glob('*.mp4'))\n",
    "    return video_files[-1] if video_files else None\n",
    "\n",
    "def extract_last_frame(video_path):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    frame = None\n",
    "    while True:\n",
    "        ret, current_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = current_frame\n",
    "    cap.release()\n",
    "    return frame\n",
    "\n",
    "def save_frame_as_image(frame, image_path):\n",
    "    cv2.imwrite(image_path, cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "def repeated_sampling(image_path, iterations=20, version=\"svd_xt\"):\n",
    "    for i in range(iterations):\n",
    "        sample(image_path, version=version)\n",
    "        output_folder = \"outputs/simple_video_sample/svd_xt/\"\n",
    "        latest_video = get_latest_video(output_folder)\n",
    "        if latest_video:\n",
    "            last_frame = extract_last_frame(latest_video)\n",
    "            image_path = f\"temp_{i}.png\"\n",
    "            save_frame_as_image(last_frame, image_path)\n",
    "\n",
    "# Initial image path\n",
    "initial_image_path = \"fire-hydrant-1.jpeg\"\n",
    "repeated_sampling(initial_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-ddrl",
   "language": "python",
   "name": "python-ddrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
